# EmCoBench
Affect computing is crucial in fields such as human-computer interaction, healthcare, and market research, yet emotion's ambiguity and subjectivity challenge current recognition techniques. We propose **Emotion Comprehension** (EC), a task that explains the reasons behind emotions, and create the **Em**otion **Co**mprehension **Bench**mark (EmCoBench) using Vision Large Language Models (VLLMs) and a VLLM-assisted dataset construction method Coarse-to-Fine Self-Ask (CFSA). EmCoBench includes 1,655 samples with 50 subsets and 78 emotion types. Experiments show limited proficiency of existing models in EC, with the best achieving 62.41% accuracy in the zero-shot setting and some performing lower than the text-only LLaMA-3 model (6.26%) in the caption-provided setting. Given its similarity to Visual Question Answering (VQA), EC can serve as a benchmark for assessing VLLMs' fine-grained emotion understanding. Addressing EC's challenges can lead to more empathetic AI systems, enhancing human-computer interaction and emotion-sensitive applications.
